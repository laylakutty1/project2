---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Layla Kutty lak2378

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information
I chose the "Breast Cancer Data" dataset because oncology is a big interest of mine as a pre-med student! I found the data on Kaggle, but it was taken from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. 

This dataset contains information related to the size of a lump when diagnosing breast cancer. The numeric variables that describe the lump are mean radius, mean texture, mean perimeter, mean area, and mean smoothness. The last variable is a binary variable known as "diagnosis" which determines whether that lump indicated the diagnosis of breast cancer. However, the data does not tell me what it was measured in. There are a total of 569 observations, 212 of which had a negative diagnosis (0) and 357 of which had a positive diagnosis (1) of breast cancer.

```{R}
library(tidyverse)
Data <- read_csv('Breast_cancer_data.csv')
```

### Cluster Analysis

```{R}
library(cluster)
sil_width <- vector()
for (i in 2:10) {
    pam_fit <- pam(Data, k = i)
    sil_width[i] <- pam_fit$silinfo$avg.width
}

ggplot() + geom_line(aes(x = 1:10, y = sil_width)) + 
    scale_x_continuous(name = "k", breaks = 1:10)
#k=2

Data_pam <- Data %>% pam(k = 2)
plot(Data_pam, which = 2)

pam1 <- Data %>% pam(k=2)
pamclust <- Data %>% mutate(cluster=as.factor(pam1$clustering))
#pamclust %>% ggplot(aes(mean_radius, mean_texture, color=cluster)) + geom_point()
#pamclust %>% ggplot(aes(mean_perimeter, mean_area, color=cluster)) + geom_point()
#pamclust %>% ggplot(aes(mean_area, mean_smoothness, color=cluster)) + geom_point()

pamclust %>% group_by(cluster) %>% summarize_if(is.numeric,mean,na.rm=T)
Data%>%slice(pam1$id.med)

library(GGally)
clust <- Data %>% mutate(cluster = as.factor(Data_pam$clustering))
clust %>% ggpairs(columns = 1:6, aes(color = cluster))
```

The PAM clustering indicates that there is a negative correlation between diagnosis and all of the other variables. This means that when the numeric variables are lower, there is a lesser chance of having a positive diagnosis for breast cancer. This would make sense! The filled in graphs also demonstrate this, as every blue cluster (diagnosis = positive) is more towards the left, while the red clusters (diagnosis = negative) is more towards the right. Additionally, the average sillhouette width was 0.69, which indicates that a reasonable structure has been found (the final fit of this cluster is reasonable).
    
### Dimensionality Reduction with PCA

```{R}
Data_nums<-Data %>% select_if(is.numeric) %>% scale
Data_pca<-princomp(Data_nums)
names(Data_pca)
summary(Data_pca, loadings=T)

Dataf<-data.frame(PC1=Data_pca$scores[, 1],PC2=Data_pca$scores[, 2])
ggplot(Dataf, aes(PC1, PC2)) + geom_point()
```

I retained PC1 and PC2 because they got me to 81% total variance explained. For PC1, if you score high overall, that means the lump has a large radius, is more textured, has a large perimeter and area, is more smooth, and is less likely to be a positive diagnosis of breast cancer. For PC2, if you score high overall, the lump is very textured and is more likely to be diagnosed as breast cancer, but the lump will be less smooth. The opposite is true if you score low for PC1 or PC2. 

###  Linear Classifier

```{R}
fit <- glm(diagnosis ~ ., data=Data)
probs <- predict(fit, type="response")
class_diag(probs, Data$diagnosis, positive=1) 
table(truth = Data$diagnosis, predictions = probs>.5)

```

```{R}
k=10 #choose number of folds
fdata<-Data[sample(nrow(Data)),] #randomly order rows
folds<-cut(seq(1:nrow(Data)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-fdata[folds!=i,] 
  test<-fdata[folds==i,]
  truth<-test$diagnosis ## Truth labels for fold i
  ## Train model on training set (all but fold i)
  fit<-glm(diagnosis~.,data=train,family="binomial")
  ## Test model on test set (fold i) 
  probs<-predict(fit,newdata = test,type="response")
  ## Get diagnostics for fold i
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}
summarize_all(diags,mean)
```

The AUC value from the linear classification is very high (0.9816), indicating that the model is doing well at predicting new observations. This is also true for the CV AUC (0.9837). Because the CV AUC was actually only a little bit higher than the other AUC, there was not overfitting, meaning it is doing a good job at predicting the data.

### Non-Parametric Classifier

```{R}
library(caret)
fit <- knn3(diagnosis ~ ., data=Data)
probs <- predict(fit, newdata=Data)[,2]
class_diag(probs, Data$diagnosis, positive=1) 
table(truth = Data$diagnosis, predictions = probs>0.5)
```

```{R}
k=10 #choose number of folds
fdata<-Data[sample(nrow(Data)),] #randomly order rows
folds<-cut(seq(1:nrow(Data)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-fdata[folds!=i,] 
  test<-fdata[folds==i,]
  truth<-test$diagnosis ## Truth labels for fold i
  ## Train model on training set (all but fold i)
  fit<-knn3(diagnosis~.,data=train)
  ## Test model on test set (fold i) 
  probs<-predict(fit,newdata = test)[,2]
  ## Get diagnostics for fold i
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}
summarize_all(diags,mean)
```

The AUC value from the linear classification is very high (0.9752), indicating that the model is doing well at predicting new observations. This is also true for the CV AUC (0.92819), however  the CV AUC was actually a little bit lower than the other AUC. This indicates signs of overfitting, meaning it may not actually be doing as good job of a job at predicting the data. Compared to the linear classification, the non-parametric classification is not doing as good of a job at predicting the data. This is seen by the fact that there is overfitting in the non-parametric classification and both AUCs are lower than the AUCs seen in the linear classification.


### Regression/Numeric Prediction

```{R}
fit <- glm(mean_radius~., data=Data)
probs <- predict(fit, type="response")
mean((Data$mean_radius-probs)^2)
```

```{R}
k=5 #choose number of folds
fdata<-Data[sample(nrow(Data)),] #randomly order rows
folds<-cut(seq(1:nrow(Data)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-fdata[folds!=i,] 
  test<-fdata[folds==i,]
  ## Fit linear regression model to training set
  fit<-lm(mean_radius~.,data=train)
  ## Get predictions/y-hats on test set (fold i)
  yhat<-predict(fit,newdata=test)
  ## Compute prediction error  (MSE) for fold i
  diags<-mean((Data$mean_radius-yhat)^2) 
}
mean(diags)
```

The MSE for the overall dataset was found to be 0.03341835. The average MSE across all of me k testing folds, however, was found to be 22.53625. This drastic difference indicates that there is definitely overfitting. This means that it is not doing a good job at predicting new observations as seen in the other classification and CV.

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3")
py$Data
```

```{python}
Data = r.Data
r.Data
```

I took the data from my environment and used r.Data to share the object in python. I also used py$Data to then share the same object in R.

### Concluding Remarks

Include concluding remarks here, if any




